{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_output": true
   },
   "source": [
    "# ELEC 576 – Introduction to Deep Learning – Assignment 2\n",
    "\n",
    "Due: 11:00am 10/25/2017\n",
    "\n",
    "*On my honor, I have neither given nor received any unauthorized aid on this assignment.*\n",
    "\n",
    "Daniel LeJeune, S01276871\n",
    "$\\newcommand{\\grenewcommand}{\\renewcommand} \\newcommand{\\gnewcommand}{\\newcommand}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.\n",
    "\n",
    "## Part (a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wellbeing/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:326: DeprecationWarning: Passing 'n_values' is deprecated in version 0.20 and will be removed in 0.22. You can use the 'categories' keyword instead. 'n_values=n' corresponds to 'categories=[range(n)]'.\n",
      "  warnings.warn(msg, DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (10000, 10), (1000, 28, 28), (1000, 10))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def load_cifar10():\n",
    "    \n",
    "    label_re = re.compile(r'.*/(\\d)/.*')\n",
    "    \n",
    "    train_coll = io.imread_collection('CIFAR10/Train/*/*.png')\n",
    "    test_coll = io.imread_collection('CIFAR10/Test/*/*.png')\n",
    "    \n",
    "    label_encoder = OneHotEncoder(10)\n",
    "    train_labels = label_encoder.fit_transform(np.array([int(label_re.match(path).group(1)) for path in train_coll.files])[:, None]).toarray()\n",
    "    test_labels = label_encoder.transform(np.array([int(label_re.match(path).group(1)) for path in test_coll.files])[:, None]).toarray()\n",
    "    \n",
    "    train_images = np.stack(train_coll).astype(float) / 255\n",
    "    test_images = np.stack(test_coll).astype(float) / 255\n",
    "    \n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "    \n",
    "X_train, y_train, X_test, y_test = load_cifar10()\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "figurescale": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-c21fb90e8486>:52: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Evaluating paramter set 1.\n",
      "Evaluating paramter set 2.\n",
      "Evaluating paramter set 3.\n",
      "Evaluating paramter set 4.\n",
      "Evaluating paramter set 5.\n",
      "Evaluating paramter set 6.\n",
      "Evaluating paramter set 7.\n",
      "Evaluating paramter set 8.\n",
      "Evaluating paramter set 9.\n",
      "Evaluating paramter set 10.\n",
      "Training with best parameters: AdamOptimizer(0.001, 0.95, epsilon=1e-08), keep_prob=0.6\n",
      "Train accuracy: 0.1000\n",
      "Test accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import ParameterSampler, train_test_split\n",
    "\n",
    "# plt.style.use('notebook')\n",
    "\n",
    "cifar10_cnn_graph = tf.Graph()\n",
    "\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "def weight(shape):\n",
    "    return tf.Variable(lambda: initializer(shape=shape))\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=(1, 1, 1, 1), padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding='SAME')\n",
    "\n",
    "with cifar10_cnn_graph.as_default():\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 28, 28])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    \n",
    "    x_one_channel = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    \n",
    "    W_conv1 = weight((5, 5, 1, 32))\n",
    "    b_conv1 = tf.constant(0.1, shape=(32,))\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_one_channel, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "    W_conv2 = weight((5, 5, 32, 64))\n",
    "    b_conv2 = tf.constant(0.1, shape=(64,))\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    W_fc1 = weight((7 * 7 * 64, 1024))\n",
    "    b_fc1 = tf.constant(0.1, shape=(1024,))\n",
    "    h_pool2_flat = tf.reshape(h_pool2, (-1, 7 * 7 * 64))\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    W_fc2 = weight((1024, 10))\n",
    "    b_fc2 = tf.constant(0.1, shape=(10,))\n",
    "    y_out = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "    \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_out))\n",
    "    correct_prediction = tf.equal(tf.argmax(y_out, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "def train_eval(X_train, y_train, X_test, y_test, graph, x, y, optimizer, epochs, batch_size, train_metric, eval_metric=None, monitor_vars=None, monitor_iters=100, train_feed_dict={}, eval_feed_dict=None):\n",
    "    \n",
    "    sess= tf.Session(graph=graph)\n",
    "    \n",
    "    with graph.as_default():\n",
    "        \n",
    "        n_train_samples = X_train.shape[0]\n",
    "        train_step = optimizer.minimize(train_metric)\n",
    "        if eval_metric is None:\n",
    "            eval_metric = train_metric\n",
    "        if eval_feed_dict is None:\n",
    "            eval_feed_dict = train_feed_dict\n",
    "        monitor_train = []\n",
    "        monitor_test = []\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        for n in range(epochs):\n",
    "            perm = np.arange(n_train_samples)\n",
    "            np.random.shuffle(perm)\n",
    "\n",
    "            for i, k in enumerate(range(0, n_train_samples, batch_size)):\n",
    "                X_batch, y_batch = X_train[perm[k:k+batch_size]], y_train[perm[k:k+batch_size]]\n",
    "                sess.run(train_step, feed_dict={x: X_batch, y: y_batch, **train_feed_dict})\n",
    "\n",
    "                if monitor_vars is not None and i % monitor_iters == 0:\n",
    "                    monitor_train.append((n*n_train_samples + k, sess.run(monitor_vars, feed_dict={x: X_train, y: y_train, **eval_feed_dict})))\n",
    "                    monitor_test.append((n*n_train_samples + k, sess.run(monitor_vars, feed_dict={x: X_test, y: y_test, **eval_feed_dict})))\n",
    "\n",
    "        eval_train = sess.run(eval_metric, feed_dict={x: X_train, y: y_train, **eval_feed_dict})\n",
    "        eval_test = sess.run(eval_metric, feed_dict={x: X_test, y: y_test, **eval_feed_dict})\n",
    "\n",
    "    if monitor_vars is None:\n",
    "        return eval_train, eval_test, sess\n",
    "    else:\n",
    "        return eval_train, eval_test, sess, monitor_train, monitor_test\n",
    "\n",
    "n_params = 10\n",
    "param_search_epochs = 5\n",
    "batch_size = 50\n",
    "full_train_epochs = 30\n",
    "full_train_monitor_iters = 100\n",
    "    \n",
    "training_params = ParameterSampler({\n",
    "    'optimizer': [tf.train.AdamOptimizer(r, b, epsilon=eps) for r, b, eps in product([1e-4, 3e-4, 1e-3], [0.8, 0.9, 0.95], [1e-8, 1e-6, 1e-4])],\n",
    "    'keep_prob': [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "}, n_iter=n_params)\n",
    "\n",
    "X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(X_train, y_train, test_size=0.1)\n",
    "\n",
    "param_search_results = []\n",
    "for i, params in enumerate(training_params):\n",
    "    print('Evaluating paramter set %d.' % (i + 1))\n",
    "    eval_train, eval_test, sess = train_eval(X_train_train, y_train_train, X_train_test, y_train_test, cifar10_cnn_graph, \n",
    "                                           x, y, params['optimizer'], param_search_epochs, batch_size, cross_entropy, accuracy,\n",
    "                                           train_feed_dict={keep_prob: params['keep_prob']}, eval_feed_dict={keep_prob: 1.0})\n",
    "    sess.close()\n",
    "    param_search_results.append((params, (eval_train, eval_test)))\n",
    "\n",
    "best_params = max(param_search_results, key=lambda x: x[1][1])[0]\n",
    "opt, p = best_params['optimizer'], best_params['keep_prob']\n",
    "\n",
    "print('Training with best parameters: AdamOptimizer(%g, %g, epsilon=%g), keep_prob=%g' % (opt._lr, opt._beta1, opt._epsilon, p))\n",
    "eval_train, eval_test, best_session, monitor_train, monitor_test = train_eval(X_train, y_train, X_test, y_test, cifar10_cnn_graph, \n",
    "                                       x, y, opt, full_train_epochs, batch_size, cross_entropy, accuracy,\n",
    "                                       train_feed_dict={keep_prob: p}, eval_feed_dict={keep_prob: 1.0},\n",
    "                                       monitor_vars=(accuracy, cross_entropy), monitor_iters=full_train_monitor_iters)\n",
    "\n",
    "print('Train accuracy: %.4f' % eval_train)\n",
    "print('Test accuracy: %.4f' % eval_test)\n",
    "\n",
    "iters, train_vals = zip(*monitor_train)\n",
    "train_accuracies, train_losses = zip(*train_vals)\n",
    "\n",
    "_, test_vals = zip(*monitor_test)\n",
    "test_accuracies, test_losses = zip(*test_vals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.array(iters) / X_train.shape[0], train_accuracies, label='Train')\n",
    "plt.plot(np.array(iters) / X_train.shape[0], test_accuracies, label='Test')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1.05)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.array(iters) / X_train.shape[0], train_losses, label='Train')\n",
    "plt.plot(np.array(iters) / X_train.shape[0], test_losses, label='Test')\n",
    "plt.title('Cross Entropy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross Entropy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dropout to the first fully connected layer and then optimizing using Adam proved to be a good strategy for training this network. \n",
    "\n",
    "## Part (c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "figurescale": 1
   },
   "outputs": [],
   "source": [
    "from matplotlib import patches as patches\n",
    "\n",
    "W1 = best_session.run(W_conv1)\n",
    "W1_std = np.std(W1.reshape((-1, W1.shape[-1])), axis=0)\n",
    "best_indices = list(np.argsort(W1_std)[::-1][:4])\n",
    "best_styles = ['solid', 'dashed', 'dashdot', 'dotted']\n",
    "\n",
    "vmax = np.abs(W1).max()\n",
    "vmin = -vmax\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "for i, j in product(range(4), range(8)):\n",
    "    k = i*8 + j\n",
    "    ax = plt.subplot(4, 8, k+1)\n",
    "    plt.pcolormesh(np.flipud(W1[:, :, 0, k]), vmin=vmin, vmax=vmax, cmap='gray')\n",
    "    if k in best_indices:\n",
    "        ax.add_patch(\n",
    "            patches.Rectangle((0, 0), 5, 5, fill=False, edgecolor='red', linewidth=3, linestyle=best_styles[best_indices.index(k)])\n",
    "        )\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "gs = plt.GridSpec(4, 8)\n",
    "for i, j in product(range(2), range(2)):\n",
    "    k = i*2 + j\n",
    "    plt.subplot(gs[i*2:i*2+2, j*4:j*4+2])\n",
    "    im = X_test[100*k+5, :, :]\n",
    "    plt.pcolormesh(np.flipud(im), vmin=0, vmax=1, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    h1 = best_session.run(h_pool1, feed_dict={x: im[None, :, :]})\n",
    "    for i2, j2 in product(range(2), range(2)):\n",
    "        k2 = i2*2 + j2\n",
    "        ax = plt.subplot(gs[i*2+i2, j*4+2+j2])\n",
    "        h = h1[0, :, :, best_indices[k2]]\n",
    "        plt.pcolormesh(np.flipud(h), cmap='gray')\n",
    "        ax.add_patch(\n",
    "            patches.Rectangle((0, 0), 14, 14, fill=False, edgecolor='red', linewidth=3, linestyle=best_styles[k2])\n",
    "        )\n",
    "        plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned convolotional weights do reasonably resemble edge detectors (top). The activations of these filters (bottom) for test images to seem to correspond to areas of the image with edges of different direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.\n",
    "\n",
    "Zeiler and Fergus present in this paper a method for visualizing the role of the hidden layers of a deep convolutional network. The method consists of a modification of a previous architecture of theirs, the Deconvolutional Network, which uses the weights of a trained convolutional network and the max-pool mappings of a forward pass to reverse construct an input-space representation of the activations of hidden layer nodes. The resulting visualizations (that is, those presented in the paper, which they say are not cherry-picked) are very intuitive and seem to shed some light on what kinds of features are actually learned by the network. This is interesting in its own right, but the authors did not stop there; like true engineers, Zeiler and Fergus exploited insight gained from these visualizations to produce a better architecture for the task of object recognition task yielding the best published performance on the ImageNet 2012 dataset at the time.\n",
    "\n",
    "In terms of hidden layer interpretations, for a network trained on the object recognition task, the first layer seemed to consist of edge and color detectors, and the second layer seemed to pick up basic textures and curves. Each layer beyond seemed to capture higher and higher levels of abstraction, from complex textures in the middle layers to actual object parts towards the highest layers. This matches the intuition that deep neural networks perform some hierarchical inference.\n",
    "\n",
    "# Task 3.\n",
    "\n",
    "## Part (a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "X_train = mnist.train.images.reshape((-1, 28, 28))\n",
    "y_train = mnist.train.labels\n",
    "X_test = mnist.test.images.reshape((-1, 28, 28))\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "def build_mnist_RNN(rnn_cell, rnn_cell_size, rnn_cell_init_copies=1):\n",
    "    \n",
    "    mnist_graph = tf.Graph()\n",
    "\n",
    "    with mnist_graph.as_default():\n",
    "\n",
    "        x = tf.placeholder(tf.float32, (None, 28, 28))\n",
    "        y = tf.placeholder(tf.float32, (None, 10))\n",
    "\n",
    "        rnn = rnn_cell(rnn_cell_size)\n",
    "\n",
    "        # from https://stackoverflow.com/a/34725458\n",
    "        state = tf.fill(tf.stack((tf.shape(x)[0], rnn_cell_size)), 0.0)\n",
    "        if rnn_cell_init_copies > 1:\n",
    "            state = (state,) * rnn_cell_init_copies\n",
    "\n",
    "        for k in range(28):\n",
    "            output, state = rnn(x[:, k, :], state)\n",
    "\n",
    "        W = weight((rnn_cell_size, 10))\n",
    "        b = weight((10,))\n",
    "\n",
    "        y_out = tf.matmul(output, W) + b\n",
    "\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_out))\n",
    "        correct_prediction = tf.equal(tf.argmax(y_out, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    MNISTVars = namedtuple('MNISTGraph', ('x', 'y', 'output', 'W', 'b', 'y_out', 'cross_entropy', 'accuracy'))\n",
    "    mnist_vars = MNISTVars(x=x, y=y, output=output, W=W, b=b, y_out=y_out, cross_entropy=cross_entropy, accuracy=accuracy)\n",
    "    \n",
    "    return mnist_graph, mnist_vars\n",
    "\n",
    "mnist_rnn_graph, mnist_rnn_vars = build_mnist_RNN(tf.contrib.rnn.BasicRNNCell, 1024)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "eval_train, eval_test, sess = train_eval(X_train, y_train, X_test, y_test, mnist_rnn_graph, \n",
    "                                         mnist_rnn_vars.x, mnist_rnn_vars.y, optimizer, epochs, batch_size, mnist_rnn_vars.cross_entropy, mnist_rnn_vars.accuracy)\n",
    "sess.close()\n",
    "print('Train accuracy: %.4f' % eval_train)\n",
    "print('Test accuracy: %.4f' % eval_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_copies_pairs = [\n",
    "    (tf.contrib.rnn.BasicRNNCell, 1), \n",
    "    (tf.contrib.rnn.BasicLSTMCell, 2), \n",
    "    (tf.contrib.rnn.GRUCell, 1)\n",
    "]\n",
    "cell_sizes = [\n",
    "    256,\n",
    "    512,\n",
    "    1024\n",
    "]\n",
    "\n",
    "monitor_iters = 100\n",
    "\n",
    "rnn_results = []\n",
    "\n",
    "for rnn_cell, init_copies in cell_copies_pairs:\n",
    "    rnn_cell_results = []\n",
    "    \n",
    "    for cell_size in cell_sizes:\n",
    "        print(rnn_cell.__name__, cell_size)\n",
    "        rnn_graph, rnn_vars = build_mnist_RNN(rnn_cell, cell_size, init_copies)\n",
    "        eval_train, eval_test, sess, monitor_train, monitor_test \\\n",
    "            = train_eval(X_train, y_train, X_test, y_test, rnn_graph, \n",
    "                         rnn_vars.x, rnn_vars.y, optimizer, epochs, batch_size, rnn_vars.cross_entropy, rnn_vars.accuracy,\n",
    "                         monitor_vars=(rnn_vars.accuracy, rnn_vars.cross_entropy), monitor_iters=monitor_iters)\n",
    "        sess.close()\n",
    "        rnn_cell_results.append((eval_train, eval_test, monitor_train, monitor_test))\n",
    "\n",
    "    rnn_results.append(rnn_cell_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "figurescale": 1
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "for j, cell_size in enumerate(cell_sizes):\n",
    "    for i, (rnn_cell, init_copies) in enumerate(cell_copies_pairs):\n",
    "\n",
    "        eval_train, eval_test, monitor_train, monitor_test = rnn_results[i][j]\n",
    "\n",
    "        iters, train_vals = zip(*monitor_train)\n",
    "        train_accuracies, train_losses = zip(*train_vals)\n",
    "\n",
    "        _, test_vals = zip(*monitor_test)\n",
    "        test_accuracies, test_losses = zip(*test_vals)\n",
    "\n",
    "        plt.subplot(3, 2, j * 2 + 1)\n",
    "        plt.plot(np.array(iters) / X_train.shape[0], train_accuracies, color=colors[i], label='%s Train' % rnn_cell.__name__)\n",
    "        plt.plot(np.array(iters) / X_train.shape[0], test_accuracies, color=colors[i], linestyle='--', label='%s Test' % rnn_cell.__name__)\n",
    "        plt.title('Accuracy (n = %d)' % cell_size)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.ylim(0, 1.05)\n",
    "\n",
    "        plt.subplot(3, 2, j * 2 + 2)\n",
    "        plt.plot(np.array(iters) / X_train.shape[0], train_losses, color=colors[i], label='%s Train' % rnn_cell.__name__)\n",
    "        plt.plot(np.array(iters) / X_train.shape[0], test_losses, color=colors[i], linestyle='--', label='%s Test' % rnn_cell.__name__)\n",
    "        plt.title('Cross Entropy (n = %d)' % cell_size)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Cross Entropy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, both the LSTM and GRU seem to perform similarly, and both of them are slightly less performant (but still very good) and slower to train than the basic RNN. This may be because there are no very long-term interactions and all sequences are of the exact same length, so the network is unable to realize the advantages of the additions of the gated cells but still suffers from their complexity. The difference in performance due to the number of hidden units also seems to be negligible; however, the networks with more hidden units seem to train faster.\n",
    "\n",
    "## Part (c).\n",
    "\n",
    "The recurrent networks were able to achieve remarkably good performance (test accuracy $\\approx 97$%), which was very close to the best I was able to achieve with a convolutional network (test accuracy $\\approx 98.7$%). It is reasonable that the recurrent networks were able to perform similarly: they are able to capture spatial relationships across rows of the images. However, they do not built the same way for the columns, which may explain the advantage the convolutional networks have. To test this, I will try an RNN over the concatenation of the $k^{th}$ row and column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mnist_RNN_double(rnn_cell, rnn_cell_size, rnn_cell_init_copies=1):\n",
    "    \n",
    "    mnist_graph = tf.Graph()\n",
    "\n",
    "    with mnist_graph.as_default():\n",
    "\n",
    "        x = tf.placeholder(tf.float32, (None, 28, 28))\n",
    "        y = tf.placeholder(tf.float32, (None, 10))\n",
    "\n",
    "        rnn = rnn_cell(rnn_cell_size, reuse=None)\n",
    "\n",
    "        # from https://stackoverflow.com/a/34725458\n",
    "        state = tf.fill(tf.stack((tf.shape(x)[0], rnn_cell_size)), 0.0)\n",
    "        if rnn_cell_init_copies > 1:\n",
    "            state = (state,) * rnn_cell_init_copies\n",
    "\n",
    "        for k in range(28):\n",
    "            output, state = rnn(tf.concat((x[:, k, :], x[:, :, k]), axis=1), state)\n",
    "            \n",
    "        W = weight((rnn_cell_size, 10))\n",
    "        b = weight((10,))\n",
    "\n",
    "        y_out = tf.matmul(output, W) + b\n",
    "\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_out))\n",
    "        correct_prediction = tf.equal(tf.argmax(y_out, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    MNISTVars = namedtuple('MNISTGraph', ('x', 'y', 'output', 'W', 'b', 'y_out', 'cross_entropy', 'accuracy'))\n",
    "    mnist_vars = MNISTVars(x=x, y=y, output=output, W=W, b=b, y_out=y_out, cross_entropy=cross_entropy, accuracy=accuracy)\n",
    "    \n",
    "    return mnist_graph, mnist_vars\n",
    "\n",
    "mnist_rnn_graph, mnist_rnn_vars = build_mnist_RNN_double(tf.contrib.rnn.BasicRNNCell, 1024)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "eval_train, eval_test, sess = train_eval(X_train, y_train, X_test, y_test, mnist_rnn_graph, \n",
    "                                         mnist_rnn_vars.x, mnist_rnn_vars.y, optimizer, epochs, batch_size, mnist_rnn_vars.cross_entropy, mnist_rnn_vars.accuracy)\n",
    "sess.close()\n",
    "print('Train accuracy: %.4f' % eval_train)\n",
    "print('Test accuracy: %.4f' % eval_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there seems to be no significant performance gain with this approach, at least not enough to be as good as the CNN."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_metadata": {
   "VARassignment": "Assignment 2",
   "VARclass": "ELEC 576 -- Introduction to Deep Learning",
   "VARdue": "11:00am 10/25/2017",
   "VARname": "Daniel LeJeune (del5, S01276871)",
   "author": "Daniel LeJeune"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
