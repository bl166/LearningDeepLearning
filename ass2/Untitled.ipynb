{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELEC 576 / COMP 576 - Fall 2018 Assignment 2\n",
    "\n",
    "## 1 Visualizing a CNN with CIFAR10\n",
    "\n",
    "### a) CIFAR10 Dataset\n",
    "\n",
    "Load CIFAR10 Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1) (10000, 10) (1000, 28, 28, 1) (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "from trainCifarStarterCode import *\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "ntrain = 1000 # per class\n",
    "ntest = 100 # per class\n",
    "nclass = 10 # number of classes\n",
    "imsize = 28\n",
    "nchannels = 1\n",
    "\n",
    "Train = np.zeros((ntrain*nclass, imsize, imsize, nchannels))\n",
    "Test = np.zeros((ntest*nclass, imsize, imsize, nchannels))\n",
    "LTrain = np.zeros((ntrain*nclass, nclass))\n",
    "LTest = np.zeros((ntest*nclass, nclass))\n",
    "\n",
    "itrain = -1\n",
    "itest = -1\n",
    "for iclass in range(0, nclass):\n",
    "    for isample in range(0, ntrain):\n",
    "        path = './CIFAR10/Train/%d/Image%05d.png' % (iclass,isample)\n",
    "        im = imageio.imread(path); # 28 by 28\n",
    "        im = im.astype(float)/255\n",
    "        itrain += 1\n",
    "        Train[itrain,:,:,0] = im\n",
    "        LTrain[itrain,iclass] = 1 # 1-hot lable\n",
    "    for isample in range(0, ntest):\n",
    "        path = './CIFAR10/Test/%d/Image%05d.png' % (iclass,isample)\n",
    "        im = imageio.imread(path); # 28 by 28\n",
    "        im = im.astype(float)/255\n",
    "        itest += 1\n",
    "        Test[itest,:,:,0] = im\n",
    "        LTest[itest,iclass] = 1 # 1-hot lable\n",
    "\n",
    "print(Train.shape, LTrain.shape, Test.shape, LTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Train LeNet5 on CIFAR10\n",
    "\n",
    "Model architecture:\n",
    "\n",
    "    • Convolutional layer with kernel 5 x 5 and 32 filter maps followed by ReLU\n",
    "    • Max Pooling layer subsampling 2\n",
    "    • Convolutional layer with kernel 5 x 5 and 64 filter maps followed by ReLU\n",
    "    • Max Pooling layer subsampling by 2\n",
    "    • Fully Connected layer that has input 7*7*64 and output 1024\n",
    "    • Fully Connected layer that has input 1024 and output 10 (for the classes)\n",
    "    • Softmax layer (Softmax Regression + Softmax Nonlinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(object):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "        # --------------------------------------------------\n",
    "        # model\n",
    "        #create your model\n",
    "\n",
    "        # first convolutional layer\n",
    "        h_pool1 = conv_layer(self.X, [5, 5, 1, 32], 'conv1', tf.nn.relu)\n",
    "\n",
    "        # second convolutional layer\n",
    "        h_pool2 = conv_layer(h_pool1, [5, 5, 32, 64], 'conv2', tf.nn.relu)\n",
    "\n",
    "        # densely connected layer\n",
    "        h_fc1 = fc_layer(h_pool2,[7 * 7 * 64, 1024], 'fc1', tf.nn.relu)\n",
    "\n",
    "        # dropout\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = dropout(h_fc1, self.keep_prob, 'drop')\n",
    "\n",
    "        # softmax\n",
    "        self.y_ = fc_layer(h_fc1_drop, [1024, 10], 'fc2', None)#tf.nn.softmax)\n",
    "\n",
    "        \n",
    "    def loss(self, learning_rate):\n",
    "        # --------------------------------------------------\n",
    "        # loss\n",
    "        #set up the loss, optimization, evaluation, and accuracy\n",
    "\n",
    "#         self.cross_entropy = tf.reduce_mean(-tf.reduce_sum(self.y * tf.log(self.y_), reduction_indices=[1]), name=\"loss_function\")\n",
    "        self.cross_entropy = tf.losses.softmax_cross_entropy(self.y, self.y_) + \\\n",
    "                             tf.losses.get_regularization_loss()\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cross_entropy)\n",
    "\n",
    "        correct_prediction = tf.cast(tf.equal(tf.argmax(self.y_,1), tf.argmax(self.y,1)), tf.float32, name=\"correct_prediction\")\n",
    "        self.accuracy = tf.reduce_mean(correct_prediction, name=\"accuracy\")\n",
    "\n",
    "        \n",
    "    def train(self, X, y, batchsize, nepoch):\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            nsamples = y.shape[0]\n",
    "            perm = np.arange(nsamples)\n",
    "            steps = np.floor(nsamples/batchsize).astype(int)\n",
    "            for i in range(nepoch): # try a small iteration size once it works then continue\n",
    "                np.random.shuffle(perm)\n",
    "                for n in range(steps):\n",
    "                    batch_ind = perm[n*batchsize:(n+1)*batchsize]\n",
    "                    self.optimizer.run(feed_dict={\n",
    "                        self.X: X[batch_ind], \n",
    "                        self.y: y[batch_ind], \n",
    "                        self.keep_prob: 0.5}) # dropout only during training\n",
    "\n",
    "                if i%10 == 0 or i==nepoch-1:\n",
    "                    #calculate train accuracy and print it\n",
    "                    train_acc, train_loss = sess.run([self.accuracy, self.cross_entropy], \n",
    "                                                     feed_dict={self.X: X[batch_ind], \n",
    "                                                                self.y: y[batch_ind], \n",
    "                                                                self.keep_prob: 1.0})\n",
    "                    test_acc, test_loss = sess.run([self.accuracy, self.cross_entropy], \n",
    "                                                   feed_dict={self.X: Test, self.y: LTest, self.keep_prob: 1.0})\n",
    "\n",
    "                    print(\"epoch %d, train accuracy %g, train loss %g, test loss %g\"%(i, train_acc, train_loss, test_loss))\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # test\n",
    "\n",
    "            print(\"test accuracy %g\"%test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train accuracy 0.2, train loss 2.19732, test loss 2.2492\n",
      "epoch 10, train accuracy 0.426667, train loss 1.69949, test loss 1.73424\n",
      "epoch 20, train accuracy 0.563333, train loss 1.37356, test loss 1.59686\n",
      "epoch 30, train accuracy 0.606667, train loss 1.2606, test loss 1.51648\n",
      "epoch 40, train accuracy 0.67, train loss 1.06117, test loss 1.4468\n",
      "epoch 50, train accuracy 0.7, train loss 0.986593, test loss 1.40889\n",
      "epoch 60, train accuracy 0.723333, train loss 0.907592, test loss 1.37879\n",
      "epoch 70, train accuracy 0.843333, train loss 0.630851, test loss 1.36931\n",
      "epoch 80, train accuracy 0.85, train loss 0.656864, test loss 1.37183\n",
      "epoch 90, train accuracy 0.863333, train loss 0.512718, test loss 1.35748\n",
      "epoch 99, train accuracy 0.883333, train loss 0.466778, test loss 1.36261\n",
      "test accuracy 0.525\n"
     ]
    }
   ],
   "source": [
    "batchSize = 300\n",
    "numberOfEpoch = 100\n",
    "learningRate = 1e-4\n",
    "\n",
    "#tf variable for the data, remember shape is [None, width, height, numberOfChannels] \n",
    "tf_data = tf.placeholder(tf.float32, [None, imsize, imsize, nchannels]) \n",
    "\n",
    "#tf variable for labels\n",
    "tf_labels = tf.placeholder(tf.float32, [None, nclass]) \n",
    "\n",
    "# --------------------------------------------------\n",
    "# optimization\n",
    "\n",
    "model = LeNet5(tf_data, tf_labels)\n",
    "model.loss(learning_rate = learningRate)\n",
    "model.train(Train, LTrain, batchSize, numberOfEpoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
